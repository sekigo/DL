{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bTqolninaIMG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def MultiHeadAttention(sekf): # основаная функция внимания\n"
      ],
      "metadata": {
        "id": "8RjsAJ71aZGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLqDuWooakMw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scales_dot_product_attention(Q,K,V, mask = None):\n",
        "  d_k = Q.shape[-1] # размерность d_k\n",
        "  K = K.transpose(-2,-1)\n",
        "  scores = torch.matmul(Q, K) / torch.sqrt(torch.tensor(d_k, dtype = torch.float32))\n",
        "\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, float('-inf')) # маскируем ненужные значения\n",
        "\n",
        "  attention = F.softmax(scores, dim = 1)\n",
        "  output = torch.matmul(attention, V)\n",
        "  return output, attention\n",
        "\n"
      ],
      "metadata": {
        "id": "UnJlDVDAaoOF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0 , \"d_model должен делиться нацело на кол-во голов\"\n",
        "\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model//num_heads   # размерность каждой головы\n",
        "\n",
        "    # линейные преобразования для Q,K,V\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # линейное преобразование проекции output\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, K, Q, V, mask = None):\n",
        "\n",
        "    batch_size = Q.shape[0]\n",
        "    Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "    K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "    V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    attention_output, attention_weights = scales_dot_product_attention(Q,K,V)\n",
        "\n",
        "\n",
        "    attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "    output = self.W_o(attention_output)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t4R4nKBffQ9I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестируем на случайных данных\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "attention = MultiHeadAttention(d_model, num_heads)\n",
        "Q = torch.rand(batch_size, seq_len, d_model)\n",
        "K = torch.rand(batch_size, seq_len, d_model)\n",
        "V = torch.rand(batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "id": "KLvkZ0dzhj7v"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dp18AtoQhzac"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, attention_w = attention(Q,K,V)\n",
        "\n",
        "print(\"Выход Multi-Head Attention:\", output.shape)  # Должно быть (batch_size, seq_len, d_model)\n",
        "print(\"Веса внимания:\", attention_w.shape)  # Должно быть (batch_size, num_heads, seq_len, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYfvfUNrhknf",
        "outputId": "b1622066-9ccd-4122-912f-1296c19df57c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выход Multi-Head Attention: torch.Size([2, 5, 512])\n",
            "Веса внимания: torch.Size([2, 8, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxMef-4ahz_m"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZlQlNlyisMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}